{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03a6e57",
   "metadata": {},
   "source": [
    "# Clone database\n",
    "\n",
    "This example will read every table of a source database and insert it into a corresponding heap table in the target database. Obviously a database is more than just heaps of data, so this is far from replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f64637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow_bcp as ab\n",
    "import pyodbc\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe0cbe-bf3f-45da-a314-002a01fffd9c",
   "metadata": {},
   "source": [
    "Database connection details need to be filled in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363992b-1fff-4171-973a-951d7f779ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_URL_SRC = ############\n",
    "SQL_URL_TGT = ############\n",
    "SQL_DB_SRC = ############\n",
    "SQL_DB_TGT = ############\n",
    "SQL_LOGIN_SRC = ############\n",
    "SQL_LOGIN_TGT = ############\n",
    "SQL_PWD_SRC = ############\n",
    "SQL_PWD_TGT = ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0e56f-9f4a-4214-91cf-3a6c3243f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyodbc_con_src = pyodbc.connect(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SQL_URL_SRC};DATABASE={SQL_DB_SRC};UID={SQL_LOGIN_SRC};PWD={SQL_PWD_SRC}\"\n",
    ")\n",
    "pyodbc_con_tgt = pyodbc.connect(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SQL_URL_TGT};DATABASE={SQL_DB_TGT};UID={SQL_LOGIN_TGT};PWD={SQL_PWD_TGT}\"\n",
    ")\n",
    "bcp_con_src = ab.ConnectionInfo(\n",
    "    (f\"-S tcp:{SQL_URL_SRC},1433 -d {SQL_DB_SRC} -U {SQL_LOGIN_SRC} -P {SQL_PWD_SRC}\").split()\n",
    ")\n",
    "bcp_con_tgt = ab.ConnectionInfo(\n",
    "    (f\"-S tcp:{SQL_URL_TGT},1433 -d {SQL_DB_TGT} -U {SQL_LOGIN_TGT} -P {SQL_PWD_TGT}\").split()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb82a6-c41d-486f-82dd-ebafb7f398a2",
   "metadata": {},
   "source": [
    "Read column types from source, will be used to replicate schema in the target database. One might also use the schema of the arrow tables to generate create scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c29abf-943f-401d-8a5a-6102c36aae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pyodbc_con_src.execute(\"\"\"\n",
    "    SELECT \n",
    "        s.name AS SchemaName, t.name AS TableName,\n",
    "        c.name AS ColumnName, c.is_nullable, ty.name AS DataType,\n",
    "        c.max_length, c.precision, c.scale\n",
    "    FROM sys.columns c\n",
    "    JOIN sys.tables t ON c.object_id = t.object_id\n",
    "    JOIN sys.schemas s ON t.schema_id = s.schema_id\n",
    "    JOIN sys.types ty ON c.user_type_id = ty.user_type_id\n",
    "    WHERE t.temporal_type <> 1\n",
    "    AND c.is_hidden = 0\n",
    "    ORDER BY s.name, t.name, c.column_id;\n",
    "\"\"\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af8c65-d75a-47b0-a390-cec445629cd1",
   "metadata": {},
   "source": [
    "Loop over tables, generate table create script and finally read data from source and write to target using arrow_bcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1bb2a1-12eb-4d9c-ac4e-00584c020ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration, ((schema_name, table_name), cols) in enumerate(itertools.groupby(schema, lambda x: x[0:2])):\n",
    "    coldefs = []\n",
    "    for _, _, name, null, dtype, charlen, prec, scale in cols:\n",
    "        size = \"\"\n",
    "        if dtype in (\"char\", \"varchar\", \"binary\", \"varbinary\"):\n",
    "            size = \"(max)\" if charlen == -1 else f\"({charlen})\"\n",
    "        if dtype in (\"nchar\", \"nvarchar\"):\n",
    "            size = \"(max)\" if charlen == -1 else f\"({charlen//2})\"\n",
    "        if dtype in (\"decimal\", \"numeric\"):\n",
    "            size = f\"({prec},{scale})\"\n",
    "        coldefs.append(f\"[{name}] {dtype}{size} NULL\")\n",
    "    create = (\n",
    "        f\"CREATE TABLE {schema_name}.{table_name} (\\n    \"\n",
    "        + \",\\n    \".join(coldefs)\n",
    "        + \"\\n);\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pyodbc_con_tgt.execute(f\"CREATE SCHEMA {schema_name}\")\n",
    "        pyodbc_con_tgt.commit()\n",
    "    except pyodbc.ProgrammingError:\n",
    "        pass\n",
    "    pyodbc_con_tgt.execute(f\"DROP TABLE IF EXISTS {schema_name}.{table_name}\")\n",
    "    pyodbc_con_tgt.commit()\n",
    "    pyodbc_con_tgt.execute(create)\n",
    "    pyodbc_con_tgt.commit()\n",
    "\n",
    "    print(f\"{iteration} finished setup: {schema_name}.{table_name}, proceeding to copy data\")\n",
    "    \n",
    "    arrow_table = bcp_con_src.download_arrow_table(f\"{schema_name}.{table_name}\")\n",
    "    bcp_con_tgt.insert_arrow(f\"{schema_name}.{table_name}\", arrow_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
